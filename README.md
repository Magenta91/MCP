# MCP
# MCP Dataset Onboarding Server  A FastAPI-based MCP (Model-Compatible Protocol) server for automating dataset onboarding using Google Drive as both input source and mock catalog.  ## Features  - **Automated Dataset Processing**: Complete workflow from raw CSV/Excel files to cataloged datasets - **Google Drive Integration**: Uses Google Drive folders as input source and catalog storage - **Metadata Extraction**: Automatically extracts column information, data types, and basic statistics - **Data Quality Rules**: Suggests DQ rules based on data characteristics - **Contract Generation**: Creates Excel contracts with schema and DQ information - **Mock Catalog**: Publishes processed artifacts to a catalog folder  ## Project Structure  ``` ‚îú‚îÄ‚îÄ main.py                    # FastAPI server and endpoints ‚îú‚îÄ‚îÄ utils.py                   # Google Drive helpers and DQ functions ‚îú‚îÄ‚îÄ dataset_processor.py       # Centralized dataset processing logic ‚îú‚îÄ‚îÄ local_test.py             # Local processing script ‚îú‚îÄ‚îÄ dataset_manager.py        # CLI tool for managing datasets ‚îú‚îÄ‚îÄ auto_processor.py         # ü§ñ Automated file monitoring ‚îú‚îÄ‚îÄ start_auto_processor.py   # üöÄ Easy startup for auto-processor ‚îú‚îÄ‚îÄ processor_dashboard.py    # üìä Monitoring dashboard ‚îú‚îÄ‚îÄ auto_config.py           # ‚öôÔ∏è Configuration management ‚îú‚îÄ‚îÄ requirements.txt          # Python dependencies ‚îú‚îÄ‚îÄ Dockerfile               # Container configuration ‚îú‚îÄ‚îÄ .env.template            # Environment variables template ‚îú‚îÄ‚îÄ processed_files.json     # üìã Auto-processor tracking log ‚îú‚îÄ‚îÄ processed_datasets/      # Organized output folder ‚îÇ   ‚îî‚îÄ‚îÄ [dataset_name]/      # Individual dataset folders ‚îÇ       ‚îú‚îÄ‚îÄ [dataset].csv    # Original dataset ‚îÇ       ‚îú‚îÄ‚îÄ [dataset]_metadata.json ‚îÇ       ‚îú‚îÄ‚îÄ [dataset]_contract.xlsx ‚îÇ       ‚îú‚îÄ‚îÄ [dataset]_dq_report.json ‚îÇ       ‚îî‚îÄ‚îÄ README.md        # Dataset summary ‚îî‚îÄ‚îÄ README.md               # This file ```  ## Setup  ### 1. Google Drive Setup  1. Create a Google Cloud Project and enable the Google Drive API 2. Create a Service Account and download the JSON key file 3. Create two Google Drive folders:    - `MCP_server`: For input CSV/Excel files    - `MCP_client`: For processed artifacts (mock catalog) 4. Share both folders with your service account email (give Editor permissions) 5. Note the folder IDs from the URLs  ### 2. Environment Configuration  1. Copy `.env.template` to `.env`:    ```bash    cp .env.template .env    ```  2. Update the `.env` file with your values:    ```env    GOOGLE_SERVICE_ACCOUNT_KEY_PATH=path/to/your/service-account-key.json    MCP_SERVER_FOLDER_ID=your_mcp_server_folder_id_here    MCP_CLIENT_FOLDER_ID=your_mcp_client_folder_id_here    HOST=0.0.0.0    PORT=8000    ```  ### 3. Installation  #### Local Development ```bash # Install dependencies pip install -r requirements.txt  # Run the server python main.py  # Or process datasets locally python local_test.py ```  #### Dataset Management CLI ```bash # List all processed datasets python dataset_manager.py list  # Process a new dataset python dataset_manager.py process YOUR_FILE_ID  # Show dataset information python dataset_manager.py info DATASET_NAME  # Clean all processed datasets python dataset_manager.py clean ```  ### ü§ñ Automated Processing (NEW!) ```bash # Start automatic file monitoring (recommended) python start_auto_processor.py  # Run single check for new files python auto_processor.py --once  # List all auto-processed files python auto_processor.py --list  # Monitor with custom interval (seconds) python auto_processor.py --interval 60  # View processing dashboard python processor_dashboard.py  # Live monitoring dashboard python processor_dashboard.py --live  # Detailed statistics python processor_dashboard.py --stats ```  #### Docker ```bash # Build the image docker build -t mcp-dataset-server .  # Run the container docker run -p 8000:8000 \   -v /path/to/your/service-account-key.json:/app/keys/service-account.json \   -e GOOGLE_SERVICE_ACCOUNT_KEY_PATH=/app/keys/service-account.json \   -e MCP_SERVER_FOLDER_ID=your_server_folder_id \   -e MCP_CLIENT_FOLDER_ID=your_client_folder_id \   mcp-dataset-server ```  ## API Endpoints  ### Core Tools  #### 1. Extract Metadata ```http POST /tool/extract_metadata Content-Type: application/json  {   "file_id": "google_drive_file_id" } ```  #### 2. Apply DQ Rules ```http POST /tool/apply_dq_rules Content-Type: application/json  {   "metadata": { ... } } ```  #### 3. Update Contract ```http POST /tool/update_contract Content-Type: application/json  {   "metadata": { ... },   "dq_rules": [ ... ] } ```  #### 4. Publish to Catalog ```http POST /tool/publish_to_catalog Content-Type: application/json  {   "metadata": { ... },   "contract_file_id": "file_id",   "dq_report": { ... } } ```  #### 5. List Catalog ```http GET /tool/list_catalog ```  ### Workflow Endpoint  #### Process Dataset (Complete Workflow) ```http POST /process_dataset Content-Type: application/json  {   "file_id": "google_drive_file_id" } ```  This endpoint runs the complete workflow: 1. Downloads CSV/Excel from `MCP_server` 2. Extracts metadata 3. Applies DQ rules 4. Creates contract.xlsx 5. Uploads all artifacts to `MCP_client`  ### Utility Endpoints  - `GET /`: Server information and available endpoints - `GET /health`: Health check with Google Drive connectivity test  ## Usage Example  1. **Upload a CSV/Excel file** to your `MCP_server` Google Drive folder 2. **Get the file ID** from the Google Drive URL 3. **Process the dataset**:    ```bash    curl -X POST "http://localhost:8000/process_dataset" \      -H "Content-Type: application/json" \      -d '{"file_id": "your_file_id_here"}'    ``` 4. **Check the catalog**:    ```bash    curl "http://localhost:8000/tool/list_catalog"    ```  ## Generated Artifacts  For each processed dataset, the following files are created in `MCP_client`:  - `{filename}_metadata.json`: Column information, data types, statistics - `{filename}_contract.xlsx`: Excel file with schema and DQ rules - `{filename}_dq_report.json`: Data quality assessment report  ## Data Quality Rules  The system automatically suggests these DQ rules:  - **Not Null**: For columns with <10% null values - **Uniqueness**: For columns with >95% unique values   - **Range Validation**: For numeric columns with min/max bounds  ## Error Handling  All endpoints include comprehensive error handling with descriptive messages. Check the response status and error details for troubleshooting.  ## Development  ### Adding New DQ Rules  Extend the `suggest_dq_rules()` function in `utils.py`:  ```python def suggest_dq_rules(metadata: Dict[str, Any]) -> List[Dict[str, Any]]:     # Add your custom DQ rule logic here     pass ```  ### Adding New Endpoints  Add new endpoints in `main.py` following the existing pattern with proper error handling and Pydantic models.  ## License  MIT License ## ü§ñ * *Automated Processing (NEW!)**  ### **Set It and Forget It** The auto-processor monitors your Google Drive folder and automatically processes new files as soon as they're uploaded - no manual intervention required!  ### **How It Works** 1. **Continuous Monitoring**: Watches your `MCP_server` Google Drive folder every 30 seconds 2. **Smart Detection**: Only processes supported files (CSV, Excel) that haven't been processed before 3. **Automatic Processing**: Runs the complete pipeline automatically 4. **Organized Storage**: Saves all artifacts in organized folders 5. **Progress Tracking**: Maintains a log of all processed files  ### **Quick Start** ```bash # Start the auto-processor (easiest way) python start_auto_processor.py  # That's it! Now just upload files to your Google Drive folder # and they'll be processed automatically ```  ### **Features** - ‚úÖ **Zero Manual Work**: Upload files and walk away - ‚úÖ **Smart File Detection**: Ignores duplicates and unsupported formats - ‚úÖ **Real-time Dashboard**: Monitor processing status - ‚úÖ **Error Recovery**: Handles failures gracefully - ‚úÖ **Detailed Logging**: Track all processing activity - ‚úÖ **Configurable**: Adjust check intervals and settings  ### **Monitoring** ```bash # View current status python processor_dashboard.py  # Live monitoring with auto-refresh python processor_dashboard.py --live  # Detailed statistics and analytics python processor_dashboard.py --stats ```  ## üéØ **Usage Scenarios**  ### **Scenario 1: Fully Automated (Recommended)** 1. Start the auto-processor: `python start_auto_processor.py` 2. Upload CSV/Excel files to your `MCP_server` Google Drive folder 3. Files are automatically processed within 30 seconds 4. Check results in `processed_datasets/` folder 5. Monitor progress with the dashboard  ### **Scenario 2: Manual Processing** 1. Upload file to Google Drive and get file ID 2. Run: `python dataset_manager.py process YOUR_FILE_ID` 3. Check results in organized folders  ### **Scenario 3: API Integration** 1. Start server: `python main.py` 2. Use REST API endpoints for programmatic access 3. Integrate with your existing data pipelines
