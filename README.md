# ğŸ¤– MCP Dataset Onboarding Server

A FastAPI-based MCP (Model-Compatible Protocol) server for automating dataset onboarding using Google Drive as both input source and mock catalog.

## ğŸ”’ **SECURITY FIRST - READ THIS BEFORE SETUP**

âš ï¸ **This repository contains template files only. You MUST configure your own credentials before use.**

ğŸ“– **Read [SECURITY_SETUP.md](SECURITY_SETUP.md) for complete security instructions.**

ğŸš¨ **Never commit service account keys or real folder IDs to version control!**

## Features

- **Automated Dataset Processing**: Complete workflow from raw CSV/Excel files to cataloged datasets
- **Google Drive Integration**: Uses Google Drive folders as input source and catalog storage
- **Metadata Extraction**: Automatically extracts column information, data types, and basic statistics
- **Data Quality Rules**: Suggests DQ rules based on data characteristics
- **Contract Generation**: Creates Excel contracts with schema and DQ information
- **Mock Catalog**: Publishes processed artifacts to a catalog folder
- **ğŸ¤– Automated Processing**: Watches folders and processes files automatically
- **ğŸŒ Multiple Interfaces**: FastAPI server, MCP server, CLI tools, and dashboards

## Project Structure

```
â”œâ”€â”€ main.py                    # FastAPI server and endpoints
â”œâ”€â”€ mcp_server.py             # True MCP protocol server for LLM integration
â”œâ”€â”€ utils.py                   # Google Drive helpers and DQ functions
â”œâ”€â”€ dataset_processor.py       # Centralized dataset processing logic
â”œâ”€â”€ auto_processor.py         # ğŸ¤– Automated file monitoring
â”œâ”€â”€ start_auto_processor.py   # ğŸš€ Easy startup for auto-processor
â”œâ”€â”€ processor_dashboard.py    # ğŸ“Š Monitoring dashboard
â”œâ”€â”€ dataset_manager.py        # CLI tool for managing datasets
â”œâ”€â”€ local_test.py             # Local processing script
â”œâ”€â”€ auto_config.py           # âš™ï¸ Configuration management
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ Dockerfile               # Container configuration
â”œâ”€â”€ .env.template            # Environment variables template
â”œâ”€â”€ .gitignore               # Security: excludes sensitive files
â”œâ”€â”€ SECURITY_SETUP.md        # ğŸ”’ Security configuration guide
â”œâ”€â”€ processed_datasets/      # Organized output folder
â”‚   â””â”€â”€ [dataset_name]/      # Individual dataset folders
â”‚       â”œâ”€â”€ [dataset].csv    # Original dataset
â”‚       â”œâ”€â”€ [dataset]_metadata.json
â”‚       â”œâ”€â”€ [dataset]_contract.xlsx
â”‚       â”œâ”€â”€ [dataset]_dq_report.json
â”‚       â””â”€â”€ README.md        # Dataset summary
â””â”€â”€ README.md               # This file
```

## ğŸš€ Quick Start

### 1. Security Setup (REQUIRED)

```bash
# 1. Read the security guide
cat SECURITY_SETUP.md

# 2. Set up your Google service account (outside this repo)
# 3. Configure your environment variables
cp .env.template .env
# Edit .env with your actual values

# 4. Verify no sensitive files will be committed
git status
```

### 2. Installation

```bash
# Install dependencies
pip install -r requirements.txt

# Test the setup
python local_test.py
```

### 3. Choose Your Interface

#### ğŸ¤– Fully Automated (Recommended)
```bash
# Start auto-processor - upload files and walk away!
python start_auto_processor.py
```

#### ğŸŒ API Server
```bash
# Start FastAPI server
python main.py
```

#### ğŸ§  LLM Integration (MCP)
```bash
# Start MCP server for Claude Desktop, etc.
python mcp_server.py
```

#### ğŸ–¥ï¸ Command Line
```bash
# Manual dataset management
python dataset_manager.py list
python dataset_manager.py process YOUR_FILE_ID
```

## ğŸ¯ Usage Scenarios

### Scenario 1: Set-and-Forget Automation
1. `python start_auto_processor.py`
2. Upload files to Google Drive
3. Files processed automatically within 30 seconds
4. Monitor with `python processor_dashboard.py --live`

### Scenario 2: LLM-Powered Data Analysis
1. Configure MCP server in Claude Desktop
2. Chat: "Analyze the dataset I just uploaded"
3. Claude uses MCP tools to process and explain your data

### Scenario 3: API Integration
1. `python main.py`
2. Integrate with your data pipelines via REST API
3. Programmatic dataset onboarding

## ğŸ“Š What You Get

For each processed dataset:
- **ğŸ“„ Original File**: Preserved in organized folder
- **ğŸ“‹ Metadata JSON**: Column info, types, statistics
- **ğŸ“Š Excel Contract**: Professional multi-sheet contract
- **ğŸ” Quality Report**: Data quality assessment
- **ğŸ“– README**: Human-readable summary

## ğŸ› ï¸ Available Tools

### FastAPI Endpoints
- `/tool/extract_metadata` - Analyze dataset structure
- `/tool/apply_dq_rules` - Generate quality rules
- `/process_dataset` - Complete workflow
- `/health` - System health check

### MCP Tools (for LLMs)
- `extract_dataset_metadata` - Dataset analysis
- `generate_data_quality_rules` - Quality assessment
- `process_complete_dataset` - Full pipeline
- `list_catalog_files` - Catalog browsing

### CLI Commands
- `dataset_manager.py list` - Show processed datasets
- `auto_processor.py --once` - Single check cycle
- `processor_dashboard.py --live` - Real-time monitoring

## ğŸ”§ Configuration

### Environment Variables (.env)
```env
GOOGLE_SERVICE_ACCOUNT_KEY_PATH=path/to/your/key.json
MCP_SERVER_FOLDER_ID=your_input_folder_id
MCP_CLIENT_FOLDER_ID=your_output_folder_id
```

### Auto-Processor Settings (auto_config.py)
- Check interval: 30 seconds
- Supported formats: CSV, Excel
- File age threshold: 1 minute
- Max files per cycle: 5

## ğŸ“ˆ Monitoring & Analytics

```bash
# Current status
python processor_dashboard.py

# Live monitoring (auto-refresh)
python processor_dashboard.py --live

# Detailed statistics
python processor_dashboard.py --stats

# Processing history
python auto_processor.py --list
```

## ğŸ³ Docker Deployment

```bash
# Build
docker build -t mcp-dataset-server .

# Run (mount your service account key securely)
docker run -p 8000:8000 \
  -v /secure/path/to/key.json:/app/keys/key.json \
  -e GOOGLE_SERVICE_ACCOUNT_KEY_PATH=/app/keys/key.json \
  -e MCP_SERVER_FOLDER_ID=your_folder_id \
  mcp-dataset-server
```

## ğŸ” Troubleshooting

### Common Issues
- **No files detected**: Check Google Drive permissions
- **Processing errors**: Verify service account access
- **MCP not working**: Check Claude Desktop configuration

### Debug Commands
```bash
# Test Google Drive connection
python -c "from utils import get_drive_service; print('âœ… Connected')"

# Check auto-processor status
python auto_processor.py --once

# Verify MCP server
python test_mcp_server.py
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. **Never commit sensitive data**
4. Test your changes
5. Submit a pull request

## ğŸ“š Documentation

- [SECURITY_SETUP.md](SECURITY_SETUP.md) - Security configuration
- [AUTOMATION_GUIDE.md](AUTOMATION_GUIDE.md) - Automation features
- [MCP_INTEGRATION_GUIDE.md](MCP_INTEGRATION_GUIDE.md) - LLM integration

## ğŸ“„ License

MIT License

## ğŸ‰ What Makes This Special

- **ğŸ”’ Security First**: Proper credential management
- **ğŸ¤– True Automation**: Zero manual intervention
- **ğŸ§  LLM Integration**: Natural language data processing
- **ğŸ“Š Professional Output**: Enterprise-ready documentation
- **ğŸ”§ Multiple Interfaces**: API, CLI, MCP, Dashboard
- **ğŸ“ˆ Real-time Monitoring**: Live processing status
- **ğŸ—‚ï¸ Perfect Organization**: Structured output folders

Transform your messy data files into professional, documented, quality-checked datasets automatically! ğŸš€